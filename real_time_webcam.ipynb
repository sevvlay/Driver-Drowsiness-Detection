{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a638922e",
   "metadata": {},
   "source": [
    "# Real-Time Driver Drowsiness Detection System\n",
    "\n",
    "This script performs live drowsiness detection using a combination of eye aspect ratio (EAR) and a deep learning model:\n",
    "\n",
    "- **Model Setup**: Loads a pretrained MobileNetV2-based model (DrowsyNet) to classify drowsiness from facial crops.\n",
    "- **Face and Landmark Detection**: Uses MediaPipe Face Mesh to extract detailed facial landmarks in real-time from webcam input.\n",
    "- **Eye Aspect Ratio (EAR)**: Calculates EAR from specific eye landmarks to detect eye closure as a simple heuristic.\n",
    "- **Drowsiness Scoring**: Maintains rolling windows for EAR-based and CNN-based detections to smooth predictions over time.\n",
    "- **Alert Logic**:\n",
    "  - When sustained signs of drowsiness (eye closure or CNN prediction) are detected for over 3 seconds, a first-level alert (\"Take a short break!\") is shown.\n",
    "  - If drowsiness persists beyond 6 seconds, a serious alert (\"Pull Over Immediately!\") is triggered.\n",
    "- **User Interface**: Real-time video feed shows the detection status with colored text and alerts.\n",
    "- **Controls**: Press 'q' to quit the application.\n",
    "\n",
    "This hybrid approach combining classical EAR metrics with a CNN model improves robustness in detecting driver fatigue and enhances road safety by timely warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a59f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from collections import deque\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "class DrowsyNet(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        base = models.mobilenet_v2(pretrained=True)\n",
    "        # Freeze all pretrained layers to avoid training them\n",
    "        for p in base.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.features = base.features\n",
    "        self.pool     = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout  = nn.Dropout(dropout_rate)\n",
    "        self.fc       = nn.Linear(base.last_channel, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model weights and set to evaluation mode\n",
    "model = DrowsyNet().to(device)\n",
    "model.load_state_dict(torch.load(\"best_drowsy_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Initialize MediaPipe Face Mesh for facial landmark detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                   refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Define image transformation pipeline for model input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Calculate Eye Aspect Ratio (EAR) to estimate eye openness\n",
    "def get_eye_ratio(landmarks, left=True):\n",
    "    if left:\n",
    "        ids = [362, 385, 387, 263, 373, 380]\n",
    "    else:\n",
    "        ids = [33, 160, 158, 133, 153, 144]\n",
    "\n",
    "    p = [np.array([landmarks[i].x, landmarks[i].y]) for i in ids]\n",
    "    A = np.linalg.norm(p[1] - p[5])\n",
    "    B = np.linalg.norm(p[2] - p[4])\n",
    "    C = np.linalg.norm(p[0] - p[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "EAR_THRESH = 0.25\n",
    "CNN_THRESH = 0.6\n",
    "FPS = 30\n",
    "SUSTAIN_FRAMES = int(FPS * 0.5)\n",
    "\n",
    "ear_win = deque(maxlen=SUSTAIN_FRAMES)\n",
    "cnn_win = deque(maxlen=SUSTAIN_FRAMES)\n",
    "\n",
    "eyes_closed_start = None\n",
    "first_warn = False\n",
    "second_warn = False\n",
    "\n",
    "last_warning_time = 0\n",
    "active_warning_text = None\n",
    "alert_level = 0  # 0: no alert, 1: first alert given\n",
    "\n",
    "# Open webcam for real-time video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe processing\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "    now = time.time()\n",
    "\n",
    "    ear_flag = False\n",
    "    cnn_flag = False\n",
    "\n",
    "    # If face landmarks detected, process eyes and run CNN prediction\n",
    "    if result.multi_face_landmarks:\n",
    "        landmarks = result.multi_face_landmarks[0].landmark\n",
    "        left_ear = get_eye_ratio(landmarks, left=True)\n",
    "        right_ear = get_eye_ratio(landmarks, left=False)\n",
    "        ear_val = (left_ear + right_ear) / 2.0\n",
    "        ear_flag = ear_val < EAR_THRESH\n",
    "        ear_win.append(1 if ear_flag else 0)\n",
    "\n",
    "        # Crop face region for CNN input based on landmarks\n",
    "        h, w, _ = frame.shape\n",
    "        x_coords = [int(landmarks[i].x * w) for i in range(33, 133)]\n",
    "        y_coords = [int(landmarks[i].y * h) for i in range(33, 133)]\n",
    "        x1, x2 = max(min(x_coords)-10, 0), min(max(x_coords)+10, w)\n",
    "        y1, y2 = max(min(y_coords)-10, 0), min(max(y_coords)+10, h)\n",
    "        face_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "        if face_crop.size:\n",
    "            inp = transform(face_crop).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model(inp)\n",
    "                probs = torch.softmax(out, 1)\n",
    "            conf, pred = probs.max(1)\n",
    "            cnn_flag = (pred.item() == 1) and (conf.item() > CNN_THRESH)\n",
    "            cnn_win.append(1 if cnn_flag else 0)\n",
    "\n",
    "    # Combine EAR and CNN detections over sustained frames\n",
    "    combined_detect = sum(ear_win) >= SUSTAIN_FRAMES or sum(cnn_win) >= SUSTAIN_FRAMES\n",
    "\n",
    "    if combined_detect:\n",
    "        if eyes_closed_start is None:\n",
    "            eyes_closed_start = now\n",
    "    else:\n",
    "        if eyes_closed_start is not None:\n",
    "            # If eyes closed longer than 3 seconds and first alert given, escalate alert\n",
    "            if now - eyes_closed_start >= 3 and alert_level == 1:\n",
    "                alert_level = 2  # Second alert condition met\n",
    "            else:\n",
    "                alert_level = 0\n",
    "        eyes_closed_start = None\n",
    "        first_warn = False\n",
    "        second_warn = False\n",
    "        active_warning_text = None\n",
    "\n",
    "    # Set alert messages based on duration of eyes closed\n",
    "    if eyes_closed_start:\n",
    "        dur = now - eyes_closed_start\n",
    "        if dur >= 6 and not second_warn:\n",
    "            active_warning_text = \"!!! SERIOUS ALERT: Pull Over Immediately !!!\"\n",
    "            last_warning_time = now\n",
    "            second_warn = True\n",
    "            alert_level = 2\n",
    "        elif dur >= 3 and not first_warn:\n",
    "            active_warning_text = \"Take a short break!\"\n",
    "            last_warning_time = now\n",
    "            first_warn = True\n",
    "            alert_level = 1\n",
    "\n",
    "    # Display alert message on screen for 3 seconds\n",
    "    if active_warning_text and now - last_warning_time < 3:\n",
    "        cv2.putText(frame, active_warning_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,165,255), 2)\n",
    "\n",
    "    status = 'normal'\n",
    "    if eyes_closed_start and (now - eyes_closed_start) >= 1:\n",
    "        status = 'DROWSY'\n",
    "    elif sum(cnn_win) >= SUSTAIN_FRAMES:\n",
    "        status = 'DROWSY'\n",
    "\n",
    "    color = (0,0,255) if status=='DROWSY' else (0,255,0)\n",
    "    cv2.putText(frame, f\"Final: {status}\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    cv2.imshow(\"MediaPipe Drowsiness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
